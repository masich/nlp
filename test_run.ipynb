{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for data\n",
    "import json\n",
    "import collections, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "## for bag-of-words, tf-idf\n",
    "from sklearn import feature_extraction \n",
    "# model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK \n",
    "**NLTK** is a leading platform for building Python programs to work with human language data.\n",
    "https://www.nltk.org/\n",
    "http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/max-\n",
      "[nltk_data]     omelchenko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[{'category': 'CRIME',\n  'headline': 'There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV',\n  'authors': 'Melissa Jeltsen',\n  'link': 'https://www.huffingtonpost.com/entry/texas-amanda-painter-mass-shooting_us_5b081ab4e4b0802d69caad89',\n  'short_description': 'She left her husband. He killed their children. Just another day in America.',\n  'date': '2018-05-26'},\n {'category': 'ENTERTAINMENT',\n  'headline': \"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\",\n  'authors': 'Andy McDonald',\n  'link': 'https://www.huffingtonpost.com/entry/will-smith-joins-diplo-and-nicky-jam-for-the-official-2018-world-cup-song_us_5b09726fe4b0fdb2aa541201',\n  'short_description': 'Of course it has a song.',\n  'date': '2018-05-26'}]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_dics = []\n",
    "with open('data/News_Category_Dataset_v2.json', mode='r', errors='ignore') as json_file:\n",
    "    for dic in json_file:\n",
    "        lst_dics.append( json.loads(dic) )\n",
    "## print the first one\n",
    "lst_dics[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "              y                                               text  \\\n60786  POLITICS                      Trump Terrifies World Leaders   \n4038   POLITICS  1 Typo Makes Sean Spicer's Rex Tillerson Tweet...   \n46137  POLITICS  HUFFPOST HILL: 'These Aren't The Gaffes You're...   \n87896  POLITICS                   War, Murder and the American Way   \n45171  POLITICS           Donald Trump Faces Payback In The Desert   \n\n                                       short_description  \n60786  President Barack Obama is trying but failing t...  \n4038   He came to praise the former secretary of stat...  \n46137  Like what you read below? Sign up for HUFFPOST...  \n87896  Today, our national numbness is wrapped in a C...  \n45171  If the presidential race in Arizona is close, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y</th>\n      <th>text</th>\n      <th>short_description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>60786</th>\n      <td>POLITICS</td>\n      <td>Trump Terrifies World Leaders</td>\n      <td>President Barack Obama is trying but failing t...</td>\n    </tr>\n    <tr>\n      <th>4038</th>\n      <td>POLITICS</td>\n      <td>1 Typo Makes Sean Spicer's Rex Tillerson Tweet...</td>\n      <td>He came to praise the former secretary of stat...</td>\n    </tr>\n    <tr>\n      <th>46137</th>\n      <td>POLITICS</td>\n      <td>HUFFPOST HILL: 'These Aren't The Gaffes You're...</td>\n      <td>Like what you read below? Sign up for HUFFPOST...</td>\n    </tr>\n    <tr>\n      <th>87896</th>\n      <td>POLITICS</td>\n      <td>War, Murder and the American Way</td>\n      <td>Today, our national numbness is wrapped in a C...</td>\n    </tr>\n    <tr>\n      <th>45171</th>\n      <td>POLITICS</td>\n      <td>Donald Trump Faces Payback In The Desert</td>\n      <td>If the presidential race in Arizona is close, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create dtf\n",
    "dtf = pd.DataFrame(lst_dics)\n",
    "## filter categories\n",
    "dtf = dtf[ dtf[\"category\"].isin(['ENTERTAINMENT','POLITICS','TECH']) ][[\"category\",\"headline\",\"short_description\"]]\n",
    "## rename columns\n",
    "dtf = dtf.rename(columns={\"category\":\"y\", \"headline\":\"text\", \"short_description\":\"short_description\"})\n",
    "## print 5 random rows\n",
    "dtf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=dtf.loc[139122,'short_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'It was day one of trying on her new brown locks as Anastasia Steele in \"Fifty Shades of Grey,\" a role that could come to'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try word_tokenize from `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'day', 'one', 'of', 'trying', 'on', 'her', 'new', 'brown', 'locks', 'as', 'Anastasia', 'Steele', 'in', '``', 'Fifty', 'Shades', 'of', 'Grey', ',', \"''\", 'a', 'role', 'that', 'could', 'come', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try `split` for tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'day', 'one', 'of', 'trying', 'on', 'her', 'new', 'brown', 'locks', 'as', 'Anastasia', 'Steele', 'in', '\"Fifty', 'Shades', 'of', 'Grey,\"', 'a', 'role', 'that', 'could', 'come', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(c.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sequence = c.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vocabulary for `token_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\"Fifty, Anastasia, Grey,\", It, Shades, Steele, a, as, brown, come, could, day, her, in, locks, new, of, on, one, role, that, to, trying, was'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(token_sequence))\n",
    "', '.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(token_sequence)\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one-hot vectors for token_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int)\n",
    "for i, word in enumerate(token_sequence):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n        0, 0]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with vocabulary and one-hot representation for each word from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    \"Fifty  Anastasia  Grey,\"  It  Shades  Steele  a  as  brown  come  ...  \\\n0        0          0       0   1       0       0  0   0      0     0  ...   \n1        0          0       0   0       0       0  0   0      0     0  ...   \n2        0          0       0   0       0       0  0   0      0     0  ...   \n3        0          0       0   0       0       0  0   0      0     0  ...   \n4        0          0       0   0       0       0  0   0      0     0  ...   \n5        0          0       0   0       0       0  0   0      0     0  ...   \n6        0          0       0   0       0       0  0   0      0     0  ...   \n7        0          0       0   0       0       0  0   0      0     0  ...   \n8        0          0       0   0       0       0  0   0      0     0  ...   \n9        0          0       0   0       0       0  0   0      1     0  ...   \n10       0          0       0   0       0       0  0   0      0     0  ...   \n11       0          0       0   0       0       0  0   1      0     0  ...   \n12       0          1       0   0       0       0  0   0      0     0  ...   \n13       0          0       0   0       0       1  0   0      0     0  ...   \n14       0          0       0   0       0       0  0   0      0     0  ...   \n15       1          0       0   0       0       0  0   0      0     0  ...   \n16       0          0       0   0       1       0  0   0      0     0  ...   \n17       0          0       0   0       0       0  0   0      0     0  ...   \n18       0          0       1   0       0       0  0   0      0     0  ...   \n19       0          0       0   0       0       0  1   0      0     0  ...   \n20       0          0       0   0       0       0  0   0      0     0  ...   \n21       0          0       0   0       0       0  0   0      0     0  ...   \n22       0          0       0   0       0       0  0   0      0     0  ...   \n23       0          0       0   0       0       0  0   0      0     1  ...   \n24       0          0       0   0       0       0  0   0      0     0  ...   \n\n    locks  new  of  on  one  role  that  to  trying  was  \n0       0    0   0   0    0     0     0   0       0    0  \n1       0    0   0   0    0     0     0   0       0    1  \n2       0    0   0   0    0     0     0   0       0    0  \n3       0    0   0   0    1     0     0   0       0    0  \n4       0    0   1   0    0     0     0   0       0    0  \n5       0    0   0   0    0     0     0   0       1    0  \n6       0    0   0   1    0     0     0   0       0    0  \n7       0    0   0   0    0     0     0   0       0    0  \n8       0    1   0   0    0     0     0   0       0    0  \n9       0    0   0   0    0     0     0   0       0    0  \n10      1    0   0   0    0     0     0   0       0    0  \n11      0    0   0   0    0     0     0   0       0    0  \n12      0    0   0   0    0     0     0   0       0    0  \n13      0    0   0   0    0     0     0   0       0    0  \n14      0    0   0   0    0     0     0   0       0    0  \n15      0    0   0   0    0     0     0   0       0    0  \n16      0    0   0   0    0     0     0   0       0    0  \n17      0    0   1   0    0     0     0   0       0    0  \n18      0    0   0   0    0     0     0   0       0    0  \n19      0    0   0   0    0     0     0   0       0    0  \n20      0    0   0   0    0     1     0   0       0    0  \n21      0    0   0   0    0     0     1   0       0    0  \n22      0    0   0   0    0     0     0   0       0    0  \n23      0    0   0   0    0     0     0   0       0    0  \n24      0    0   0   0    0     0     0   1       0    0  \n\n[25 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>\"Fifty</th>\n      <th>Anastasia</th>\n      <th>Grey,\"</th>\n      <th>It</th>\n      <th>Shades</th>\n      <th>Steele</th>\n      <th>a</th>\n      <th>as</th>\n      <th>brown</th>\n      <th>come</th>\n      <th>...</th>\n      <th>locks</th>\n      <th>new</th>\n      <th>of</th>\n      <th>on</th>\n      <th>one</th>\n      <th>role</th>\n      <th>that</th>\n      <th>to</th>\n      <th>trying</th>\n      <th>was</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>25 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(onehot_vectors, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of 'POLITICS' news from dtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(dtf[dtf['y']=='POLITICS']['short_description'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Last month a Health and Human Services official revealed the government was unable to locate nearly 1,500 children who had been released from its custody.',\n 'The wiretaps feature conversations between Alexander Torshin and Alexander Romanov, a convicted Russian money launderer.',\n \"But don't count on Robert Mueller to nail him, the NSA whistleblower warns.\",\n 'Just a peeping minute.',\n 'Irish women will no longer have to travel to the United Kingdom to end their pregnancies.',\n 'The interior secretary attempts damage control with hunting and fishing groups that didn’t like his fossil fuel focus.',\n 'And there are four times as many male as female executives.',\n 'A new law to fight sex trafficking targets some of the people it ostensibly aims to protect.',\n 'For Curry and others, being Christian means rejecting white nationalism and misogyny, while protecting immigrants, refugees and the poor.',\n 'The Chinese Exclusion Act barely gets mentioned in U.S. history classes. A new PBS documentary from directors Ric Burns and Li-Shin Yu could change that.',\n 'Stone\\'s contact with Julian Assange reportedly told him he couldn\\'t ask WikiLeaks favors \"every other day.\"',\n '\"Publix knows we\\'re not going away,\" one gun-control activist said.',\n '“Our results suggest that, given narrow margins of victories in each vote, bots’ effect was likely marginal but possibly large enough to affect the outcomes.\"',\n 'A videographer made fun of the far-right extremist group online, so they showed up at his house.',\n \"Godlessness. Too many doors in schools. Ritalin. Guess what's not on the list?\",\n 'Catholic hospital systems are pushing Trump to expand “conscience rights” – even though faith-based medicine has nearly killed some of their patients.',\n \"Unions denounced the president's actions an “assault on democracy.”\",\n '\"The process of writing this book has been so personally meaningful and illuminating for me.\"',\n '\"Trump\\'s more concerned about Tomi Lahren being splashed with water than Flint still being entirely without safe water.\"',\n 'Trump cited North Korea’s “tremendous anger and open hostility” in calling off the June 12 Singapore meeting.']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Stop words\" processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/max-\n",
      "[nltk_data]     omelchenko/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Irish', 'women', 'will', 'no', 'longer', 'have', 'to', 'travel', 'to', 'the', 'United', 'Kingdom', 'to', 'end', 'their', 'pregnancies', '.']\n",
      "['Irish', 'women', 'longer', 'travel', 'United', 'Kingdom', 'end', 'pregnancies', '.']\n"
     ]
    }
   ],
   "source": [
    "# random sentecnce with lot of stop words\n",
    "sample_text = corpus[4]\n",
    "text_tokens = word_tokenize(sample_text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "\n",
    "print(text_tokens)\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('Irish', 'women'),\n ('women', 'will'),\n ('will', 'no'),\n ('no', 'longer'),\n ('longer', 'have'),\n ('have', 'to'),\n ('to', 'travel'),\n ('travel', 'to'),\n ('to', 'the'),\n ('the', 'United'),\n ('United', 'Kingdom'),\n ('Kingdom', 'to'),\n ('to', 'end'),\n ('end', 'their'),\n ('their', 'pregnancies'),\n ('pregnancies', '.')]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(text_tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words by using `CountVectorizer()` from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   '12': 1,\n",
      "    '500': 1,\n",
      "    'about': 1,\n",
      "    'act': 1,\n",
      "    'actions': 1,\n",
      "    'activist': 1,\n",
      "    'affect': 1,\n",
      "    'aims': 1,\n",
      "    'alexander': 2,\n",
      "    'an': 1,\n",
      "    'and': 10,\n",
      "    'anger': 1,\n",
      "    'are': 2,\n",
      "    'as': 2,\n",
      "    'ask': 1,\n",
      "    'assange': 1,\n",
      "    'assault': 1,\n",
      "    'at': 1,\n",
      "    'attempts': 1,\n",
      "    'away': 1,\n",
      "    'barely': 1,\n",
      "    'based': 1,\n",
      "    'been': 2,\n",
      "    'being': 3,\n",
      "    'between': 1,\n",
      "    'book': 1,\n",
      "    'bots': 1,\n",
      "    'burns': 1,\n",
      "    'but': 2,\n",
      "    'calling': 1,\n",
      "    'catholic': 1,\n",
      "    'change': 1,\n",
      "    'children': 1,\n",
      "    'chinese': 1,\n",
      "    'christian': 1,\n",
      "    'cited': 1,\n",
      "    'classes': 1,\n",
      "    'concerned': 1,\n",
      "    'conscience': 1,\n",
      "    'contact': 1,\n",
      "    'control': 2,\n",
      "    'conversations': 1,\n",
      "    'convicted': 1,\n",
      "    'could': 1,\n",
      "    'couldn': 1,\n",
      "    'count': 1,\n",
      "    'curry': 1,\n",
      "    'custody': 1,\n",
      "    'damage': 1,\n",
      "    'day': 1,\n",
      "    'democracy': 1,\n",
      "    'denounced': 1,\n",
      "    'didn': 1,\n",
      "    'directors': 1,\n",
      "    'documentary': 1,\n",
      "    'don': 1,\n",
      "    'doors': 1,\n",
      "    'each': 1,\n",
      "    'effect': 1,\n",
      "    'end': 1,\n",
      "    'enough': 1,\n",
      "    'entirely': 1,\n",
      "    'even': 1,\n",
      "    'every': 1,\n",
      "    'exclusion': 1,\n",
      "    'executives': 1,\n",
      "    'expand': 1,\n",
      "    'extremist': 1,\n",
      "    'faith': 1,\n",
      "    'far': 1,\n",
      "    'favors': 1,\n",
      "    'feature': 1,\n",
      "    'female': 1,\n",
      "    'fight': 1,\n",
      "    'fishing': 1,\n",
      "    'flint': 1,\n",
      "    'focus': 1,\n",
      "    'for': 2,\n",
      "    'fossil': 1,\n",
      "    'four': 1,\n",
      "    'from': 2,\n",
      "    'fuel': 1,\n",
      "    'fun': 1,\n",
      "    'gets': 1,\n",
      "    'given': 1,\n",
      "    'godlessness': 1,\n",
      "    'going': 1,\n",
      "    'government': 1,\n",
      "    'group': 1,\n",
      "    'groups': 1,\n",
      "    'guess': 1,\n",
      "    'gun': 1,\n",
      "    'had': 1,\n",
      "    'has': 2,\n",
      "    'have': 1,\n",
      "    'he': 1,\n",
      "    'health': 1,\n",
      "    'him': 2,\n",
      "    'his': 2,\n",
      "    'history': 1,\n",
      "    'hospital': 1,\n",
      "    'hostility': 1,\n",
      "    'house': 1,\n",
      "    'human': 1,\n",
      "    'hunting': 1,\n",
      "    'illuminating': 1,\n",
      "    'immigrants': 1,\n",
      "    'in': 4,\n",
      "    'interior': 1,\n",
      "    'irish': 1,\n",
      "    'it': 1,\n",
      "    'its': 1,\n",
      "    'julian': 1,\n",
      "    'june': 1,\n",
      "    'just': 1,\n",
      "    'killed': 1,\n",
      "    'kingdom': 1,\n",
      "    'knows': 1,\n",
      "    'korea': 1,\n",
      "    'lahren': 1,\n",
      "    'large': 1,\n",
      "    'last': 1,\n",
      "    'launderer': 1,\n",
      "    'law': 1,\n",
      "    'li': 1,\n",
      "    'like': 1,\n",
      "    'likely': 1,\n",
      "    'list': 1,\n",
      "    'locate': 1,\n",
      "    'longer': 1,\n",
      "    'made': 1,\n",
      "    'male': 1,\n",
      "    'many': 2,\n",
      "    'marginal': 1,\n",
      "    'margins': 1,\n",
      "    'me': 1,\n",
      "    'meaningful': 1,\n",
      "    'means': 1,\n",
      "    'medicine': 1,\n",
      "    'meeting': 1,\n",
      "    'mentioned': 1,\n",
      "    'minute': 1,\n",
      "    'misogyny': 1,\n",
      "    'money': 1,\n",
      "    'month': 1,\n",
      "    'more': 1,\n",
      "    'mueller': 1,\n",
      "    'nail': 1,\n",
      "    'narrow': 1,\n",
      "    'nationalism': 1,\n",
      "    'nearly': 2,\n",
      "    'new': 2,\n",
      "    'no': 1,\n",
      "    'north': 1,\n",
      "    'not': 2,\n",
      "    'nsa': 1,\n",
      "    'of': 5,\n",
      "    'off': 1,\n",
      "    'official': 1,\n",
      "    'on': 3,\n",
      "    'one': 1,\n",
      "    'online': 1,\n",
      "    'open': 1,\n",
      "    'ostensibly': 1,\n",
      "    'other': 1,\n",
      "    'others': 1,\n",
      "    'our': 1,\n",
      "    'outcomes': 1,\n",
      "    'patients': 1,\n",
      "    'pbs': 1,\n",
      "    'peeping': 1,\n",
      "    'people': 1,\n",
      "    'personally': 1,\n",
      "    'poor': 1,\n",
      "    'possibly': 1,\n",
      "    'pregnancies': 1,\n",
      "    'president': 1,\n",
      "    'process': 1,\n",
      "    'protect': 1,\n",
      "    'protecting': 1,\n",
      "    'publix': 1,\n",
      "    'pushing': 1,\n",
      "    're': 1,\n",
      "    'refugees': 1,\n",
      "    'rejecting': 1,\n",
      "    'released': 1,\n",
      "    'reportedly': 1,\n",
      "    'results': 1,\n",
      "    'revealed': 1,\n",
      "    'ric': 1,\n",
      "    'right': 1,\n",
      "    'rights': 1,\n",
      "    'ritalin': 1,\n",
      "    'robert': 1,\n",
      "    'romanov': 1,\n",
      "    'russian': 1,\n",
      "    'safe': 1,\n",
      "    'said': 1,\n",
      "    'schools': 1,\n",
      "    'secretary': 1,\n",
      "    'services': 1,\n",
      "    'sex': 1,\n",
      "    'shin': 1,\n",
      "    'showed': 1,\n",
      "    'singapore': 1,\n",
      "    'so': 2,\n",
      "    'some': 2,\n",
      "    'splashed': 1,\n",
      "    'still': 1,\n",
      "    'stone': 1,\n",
      "    'suggest': 1,\n",
      "    'systems': 1,\n",
      "    'targets': 1,\n",
      "    'than': 1,\n",
      "    'that': 3,\n",
      "    'the': 14,\n",
      "    'their': 2,\n",
      "    'there': 1,\n",
      "    'they': 1,\n",
      "    'this': 1,\n",
      "    'though': 1,\n",
      "    'times': 1,\n",
      "    'to': 9,\n",
      "    'told': 1,\n",
      "    'tomi': 1,\n",
      "    'too': 1,\n",
      "    'torshin': 1,\n",
      "    'trafficking': 1,\n",
      "    'travel': 1,\n",
      "    'tremendous': 1,\n",
      "    'trump': 3,\n",
      "    'unable': 1,\n",
      "    'unions': 1,\n",
      "    'united': 1,\n",
      "    'up': 1,\n",
      "    'victories': 1,\n",
      "    'videographer': 1,\n",
      "    'vote': 1,\n",
      "    'warns': 1,\n",
      "    'was': 2,\n",
      "    'water': 2,\n",
      "    'we': 1,\n",
      "    'what': 1,\n",
      "    'while': 1,\n",
      "    'whistleblower': 1,\n",
      "    'white': 1,\n",
      "    'who': 1,\n",
      "    'wikileaks': 1,\n",
      "    'will': 1,\n",
      "    'wiretaps': 1,\n",
      "    'with': 3,\n",
      "    'without': 1,\n",
      "    'women': 1,\n",
      "    'writing': 1,\n",
      "    'yu': 1}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word_list = vectorizer.get_feature_names()\n",
    "count_list = X.toarray().sum(axis=0)    \n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(dict(zip(word_list,count_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Counter() from collection for Bag of Words creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagofwords = [collections.Counter(re.findall(r'\\w+', txt)) for txt in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({'Last': 1,\n         'month': 1,\n         'a': 1,\n         'Health': 1,\n         'and': 1,\n         'Human': 1,\n         'Services': 1,\n         'official': 1,\n         'revealed': 1,\n         'the': 1,\n         'government': 1,\n         'was': 1,\n         'unable': 1,\n         'to': 1,\n         'locate': 1,\n         'nearly': 1,\n         '1': 1,\n         '500': 1,\n         'children': 1,\n         'who': 1,\n         'had': 1,\n         'been': 1,\n         'released': 1,\n         'from': 1,\n         'its': 1,\n         'custody': 1})"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagofwords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({'The': 1,\n         'wiretaps': 1,\n         'feature': 1,\n         'conversations': 1,\n         'between': 1,\n         'Alexander': 2,\n         'Torshin': 1,\n         'and': 1,\n         'Romanov': 1,\n         'a': 1,\n         'convicted': 1,\n         'Russian': 1,\n         'money': 1,\n         'launderer': 1})"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagofwords[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({'But': 1,\n         'don': 1,\n         't': 1,\n         'count': 1,\n         'on': 1,\n         'Robert': 1,\n         'Mueller': 1,\n         'to': 1,\n         'nail': 1,\n         'him': 1,\n         'the': 1,\n         'NSA': 1,\n         'whistleblower': 1,\n         'warns': 1})"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagofwords[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example with tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The wiretaps feature conversations between Alexander Torshin and Alexander Romanov, a convicted Russian money launderer.', 'For Curry and others, being Christian means rejecting white nationalism and misogyny, while protecting immigrants, refugees and the poor.', \"Unions denounced the president's actions an “assault on democracy.”\"]\n"
     ]
    }
   ],
   "source": [
    "s0=corpus[1]\n",
    "s1 = corpus[8]\n",
    "s2=corpus[16]\n",
    "\n",
    "sent=[s0, s1, s2]\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "response = vectorizer.fit_transform([s0,s1, s2])\n",
    "features=vectorizer.get_feature_names()\n",
    "x=response.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actions', 'alexander', 'an', 'and', 'assault', 'being', 'between', 'christian', 'conversations', 'convicted', 'curry', 'democracy', 'denounced', 'feature', 'for', 'immigrants', 'launderer', 'means', 'misogyny', 'money', 'nationalism', 'on', 'others', 'poor', 'president', 'protecting', 'refugees', 'rejecting', 'romanov', 'russian', 'the', 'torshin', 'unions', 'while', 'white', 'wiretaps']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    actions  alexander        an       and   assault    being   between  \\\n0  0.000000   0.517655  0.000000  0.196845  0.000000  0.00000  0.258828   \n1  0.000000   0.000000  0.000000  0.503249  0.000000  0.22057  0.000000   \n2  0.346089   0.000000  0.346089  0.000000  0.346089  0.00000  0.000000   \n\n   christian  conversations  convicted  ...  refugees  rejecting   romanov  \\\n0    0.00000       0.258828   0.258828  ...   0.00000    0.00000  0.258828   \n1    0.22057       0.000000   0.000000  ...   0.22057    0.22057  0.000000   \n2    0.00000       0.000000   0.000000  ...   0.00000    0.00000  0.000000   \n\n    russian       the   torshin    unions    while    white  wiretaps  \n0  0.258828  0.152868  0.258828  0.000000  0.00000  0.00000  0.258828  \n1  0.000000  0.130272  0.000000  0.000000  0.22057  0.22057  0.000000  \n2  0.000000  0.204405  0.000000  0.346089  0.00000  0.00000  0.000000  \n\n[3 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>actions</th>\n      <th>alexander</th>\n      <th>an</th>\n      <th>and</th>\n      <th>assault</th>\n      <th>being</th>\n      <th>between</th>\n      <th>christian</th>\n      <th>conversations</th>\n      <th>convicted</th>\n      <th>...</th>\n      <th>refugees</th>\n      <th>rejecting</th>\n      <th>romanov</th>\n      <th>russian</th>\n      <th>the</th>\n      <th>torshin</th>\n      <th>unions</th>\n      <th>while</th>\n      <th>white</th>\n      <th>wiretaps</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.517655</td>\n      <td>0.000000</td>\n      <td>0.196845</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.258828</td>\n      <td>0.00000</td>\n      <td>0.258828</td>\n      <td>0.258828</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.258828</td>\n      <td>0.258828</td>\n      <td>0.152868</td>\n      <td>0.258828</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.258828</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.503249</td>\n      <td>0.000000</td>\n      <td>0.22057</td>\n      <td>0.000000</td>\n      <td>0.22057</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.22057</td>\n      <td>0.22057</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.130272</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.22057</td>\n      <td>0.22057</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.346089</td>\n      <td>0.000000</td>\n      <td>0.346089</td>\n      <td>0.000000</td>\n      <td>0.346089</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.204405</td>\n      <td>0.000000</td>\n      <td>0.346089</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=x, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2: actions - 0.3460885720028406\n"
     ]
    }
   ],
   "source": [
    "print('s2:',features[0],'-', response[(2,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s0: alexander - 0.5176550192273711\n"
     ]
    }
   ],
   "source": [
    "print('s0:',features[1],'-', response[(0,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1: christian - 0.22057046589571722\n"
     ]
    }
   ],
   "source": [
    "print('s1:',features[7],'-', response[(1,7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=response.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.        , 0.51765502, 0.        , 0.19684499, 0.        ,\n       0.        , 0.25882751, 0.        , 0.25882751, 0.25882751,\n       0.        , 0.        , 0.        , 0.25882751, 0.        ,\n       0.        , 0.25882751, 0.        , 0.        , 0.25882751,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.25882751, 0.25882751,\n       0.1528677 , 0.25882751, 0.        , 0.        , 0.        ,\n       0.25882751])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.        , 0.        , 0.        , 0.50324857, 0.        ,\n       0.22057047, 0.        , 0.22057047, 0.        , 0.        ,\n       0.22057047, 0.        , 0.        , 0.        , 0.22057047,\n       0.22057047, 0.        , 0.22057047, 0.22057047, 0.        ,\n       0.22057047, 0.        , 0.22057047, 0.22057047, 0.        ,\n       0.22057047, 0.22057047, 0.22057047, 0.        , 0.        ,\n       0.13027247, 0.        , 0.        , 0.22057047, 0.22057047,\n       0.        ])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.34608857, 0.        , 0.34608857, 0.        , 0.34608857,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.34608857, 0.34608857, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.34608857, 0.        , 0.        , 0.34608857,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.20440549, 0.        , 0.34608857, 0.        , 0.        ,\n       0.        ])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate cosine similarity for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "cos_sim = np.dot(x[0], x[1])/(norm(x[0])*norm(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.11897641420763513"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.031246995803410012"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim2 = np.dot(x[0], x[2])/(norm(x[0])*norm(x[2]))\n",
    "cos_sim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.02662840759271147"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim3 = np.dot(x[1], x[2])/(norm(x[1])*norm(x[2]))\n",
    "cos_sim3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}